Implementation Guide: Parameter Scaling in Harris et al. (2023)In implementing the model described in Harris et al. (2023): Effect of sparsity on network stability in random neural networks obeying Dale's law, the correct approach is to scale the individual population parameters ($\mu_E, \mu_I, \sigma_E, \sigma_I$) by $1/\sqrt{N}$.1. The Scaling RuleThe authors explicitly adopt a convention from Random Matrix Theory (RMT) to ensure the eigenspectrum remains stable in the thermodynamic limit ($N \to \infty$). Specifically, they define the scaled variables as:$$\tilde{\mu} = \frac{\mu}{\sqrt{N}} \quad \text{and} \quad \tilde{\sigma} = \frac{\sigma}{\sqrt{N}}$$When applying Dale's Law (Section III.D), these scaled variables are assigned to the excitatory and inhibitory populations:Excitatory: sampled from $\mathcal{N}(\tilde{\mu}_e, \tilde{\sigma}_e^2)$Inhibitory: sampled from $\mathcal{N}(\tilde{\mu}_i, \tilde{\sigma}_i^2)$2. Mathematical JustificationThe scaling of individual parameters ensures that the primary features of the eigenspectrum (the outlier and the bulk disk) scale predictably with $N$.The Eigenvalue Outlier ($\lambda_O$)The outlier represents the "mean" behavior of the network connectivity. Based on Appendix B, the mean of the entries in the sparse matrix $W$ is:$$\mathbb{E}(w_{ij}) = f \alpha \tilde{\mu}_e + (1-f) \alpha \tilde{\mu}_i$$The outlier is then:$$\lambda_O = N \cdot \mathbb{E}(w_{ij}) = \sqrt{N} [f \alpha \mu_e + (1-f) \alpha \mu_i]$$By scaling $\mu$ by $1/\sqrt{N}$, the outlier grows as $O(\sqrt{N})$, which is the standard regime for studying stability transitions in these networks.The Eigenspectral Radius ($\mathcal{R}$)The radius of the bulk disk depends on the variance of the entries. For each population $k \in \{e, i\}$, the variance $\sigma_{sk}^2$ is derived as:$$\sigma_{sk}^2 = \alpha(1-\alpha)\tilde{\mu}_k^2 + \alpha\tilde{\sigma}_k^2$$Substituting the scaled variables:$$\sigma_{sk}^2 = \alpha(1-\alpha) \left(\frac{\mu_k}{\sqrt{N}}\right)^2 + \alpha \left(\frac{\sigma_k}{\sqrt{N}}\right)^2$$The total radius is $\mathcal{R} = \sqrt{N \cdot \mathbb{V}ar(W)}$. Because the variance contains a $1/N$ factor from the scaled parameters, the $N$ cancels out, resulting in a radius that is $O(1)$:$$\mathcal{R} = \sqrt{f[\alpha(1-\alpha)\mu_e^2 + \alpha\sigma_e^2] + (1-f)[\alpha(1-\alpha)\mu_i^2 + \alpha\sigma_i^2]}$$3. Implementation StepsWhen writing your simulation code, follow these steps:Define Constants: Choose your unscaled "physical" parameters (e.g., $\mu_e = 1.0$, $\mu_i = -4.0$).Apply Scaling: Divide these values by $\sqrt{N}$ before populating the matrix:mu_tilde_e = mu_e / np.sqrt(N)
sigma_tilde_e = sigma_e / np.sqrt(N)
Construct Matrix: Use these $\tilde{\mu}$ and $\tilde{\sigma}$ values to fill the non-zero entries of your sparse connectivity matrix $W$.4. Why This MattersIf you do not scale the individual population means and variances by $1/\sqrt{N}$:Stability: The radius $\mathcal{R}$ will grow as $\sqrt{N}$. In large networks, the disk will always cross the stability boundary, making it impossible to find a stable regime for large $N$.Saturation: The outlier $\lambda_O$ will grow linearly with $N$, likely causing the network activity to diverge or saturate the firing rate function immediately.